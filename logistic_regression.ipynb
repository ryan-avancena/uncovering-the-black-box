{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# machine learning - logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, X, y):\n",
    "        self.X = np.array(X)\n",
    "        self.Y = np.array(y)\n",
    "        self.num_attributes = self.X.shape[1]\n",
    "        self.coef_ = np.zeros(self.num_attributes)\n",
    "        self.intercept_ = 0\n",
    "\n",
    "    def sig(self,v):\n",
    "        return 1/(1 + np.exp(-v))\n",
    "\n",
    "    # binary cross entropy\n",
    "    def cost_function(self):\n",
    "        m = self.X.shape[0]     # number of samples\n",
    "\n",
    "\n",
    "        \"\"\" y_pred\n",
    "\n",
    "        - self.X @ self.coef_ is matrix multiplication between our feature matrix and our weight matrix \n",
    "        - self.intercept_ is just us adding bias\n",
    "        - similar to linear regression but constrained to a value between 0,1\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        y_pred = self.sig(self.X @ self.coef_ + self.intercept_)    # predictions\n",
    "        return - (1/m) * np.sum(self.Y * np.log(y_pred) + (1 - self.Y) * np.log(1 - y_pred))\n",
    "\n",
    "    # gradient descent\n",
    "    def fit(self, lr=0.01, epochs=1000):\n",
    "        m = self.X.shape[0]\n",
    "\n",
    "        for _ in range(epochs):\n",
    "            y_pred = self.sig(self.X @ self.coef_ + self.intercept_)\n",
    "            error = y_pred - self.Y     # error\n",
    "\n",
    "            # gradients\n",
    "            dW = (1/m) * (self.X.T @ error)\n",
    "            dB = (1/m) * (np.sum(error))\n",
    "\n",
    "            # updating the parameters\n",
    "            self.coef_ -= lr * dW\n",
    "            self.intercept_ -= lr * dB\n",
    " \n",
    "\n",
    "    def predict(self, X_new):\n",
    "        y_pred = self.sig(X_new @ self.coef_ + self.intercept_)\n",
    "        return (y_pred >= 0.5).astype(int)  # convert probabilities to class labels\n",
    "\n",
    "\n",
    "    def score(self, X_test, y_test):\n",
    "        y_pred = self.predict(X_test)\n",
    "        return np.mean(y_pred == y_test)  # accuracy calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.0\n",
      "Predictions for [[0,0], [1,1]]: [0 1]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([0, 1, 1, 0])  # XOR labels\n",
    "\n",
    "X_train = X[:2]\n",
    "X_test = X[2:]\n",
    "y_train = y[:2]\n",
    "y_test = y[2:]\n",
    "\n",
    "mean = np.mean(X, axis=0)\n",
    "std = np.std(X, axis=0)\n",
    "\n",
    "std[std == 0] = 1\n",
    "\n",
    "X_train = (X_train - mean) / std\n",
    "X_test = (X_test - mean) / std\n",
    "\n",
    "model = LogisticRegression(X_train, y_train)\n",
    "model.fit(lr=0.1, epochs=5000)\n",
    "\n",
    "accuracy = model.score(X_test, y_test)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "# Predict on new data\n",
    "new_data = np.array([[0, 0], [1, 1]])\n",
    "new_data = (new_data - mean) / std  \n",
    "predictions = model.predict(new_data)\n",
    "print(\"Predictions for [[0,0], [1,1]]:\", predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
